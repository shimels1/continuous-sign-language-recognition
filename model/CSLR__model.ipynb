{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "editing": false,
          "id": "1d30594f-5999-402d-a72b-797ab857b371",
          "kernelId": ""
        },
        "id": "g85gT4vmW5JL",
        "outputId": "87d2af85-a6ab-411b-b2d4-e23cc3d9d9dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "editing": false,
          "id": "d5a65d30-db45-447b-8c09-b413ef14d57d",
          "kernelId": ""
        },
        "id": "6v4ZlimfGnJ9",
        "outputId": "9c604396-8530-4822-f71f-8205d0de421d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PwKJhL-_GIQ2r__khhdCrk6JKXuwST6o\n",
            "To: /content/dataset_v6.csv\n",
            "100% 156M/156M [00:00<00:00, 190MB/s]\n"
          ]
        }
      ],
      "source": [
        "#download dataset from google drive\n",
        "!gdown --id 1PwKJhL-_GIQ2r__khhdCrk6JKXuwST6o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "editing": false,
          "id": "8e92cff1-8ed3-4b90-9ca3-ff22e535211a",
          "kernelId": ""
        },
        "id": "PpeIodGZdRxs",
        "outputId": "d90f2c85-053e-4e14-b1a1-df140af8a073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "editing": false,
          "id": "bc2d9e49-0bfb-4fb0-8813-965c6e54a0e6",
          "kernelId": ""
        },
        "id": "4AevmX7kdRxu",
        "outputId": "732d784d-d317-4ace-a7d1-32402cffc9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "editing": false,
          "id": "f4e262f6-a93f-40a2-b9a8-c5cf959deeb7",
          "kernelId": ""
        },
        "id": "kfamTizedRxv",
        "outputId": "e4bf622d-f86c-4bba-ffcb-275c9922268b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "editing": false,
          "id": "56fbbfba-4980-40f5-bece-ea1a3a62005c",
          "kernelId": ""
        },
        "id": "lmaI3sthdRxw",
        "outputId": "26db81fa-e0fa-4b30-d4af-fdbd7f1ea193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-2.3.0-py3-none-any.whl (15 kB)\n",
            "Collecting python-Levenshtein==0.12.2\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein==0.12.2->jiwer) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149855 sha256=e5e7ef9d7f1f6bed8734af8ba7110bc7e5a093bce5e776d8fb80e33bf074933f\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein, jiwer\n",
            "Successfully installed jiwer-2.3.0 python-Levenshtein-0.12.2\n"
          ]
        }
      ],
      "source": [
        "pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "e3c89f10-bf9a-4395-b655-bf990e7cb6fd",
          "kernelId": ""
        },
        "id": "0MTSHhKI_ovI"
      },
      "outputs": [],
      "source": [
        "# Importing libraries and packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from jiwer import wer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Input,Lambda\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import  Model\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "editing": false,
          "id": "7f46789d-eca5-43b2-94ec-de521313c09c",
          "kernelId": ""
        },
        "id": "YwptEMQOdR9z",
        "outputId": "8ab66950-a0c8-4aa1-c8fc-19e68b26e503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13041, 2049)\n"
          ]
        }
      ],
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the training dataset\n",
        "dataset_train = pd.read_csv('dataset_v6.csv')\n",
        "print(dataset_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "59b74a89-5a2a-4b7a-a2d0-9fa9b81e8c8c",
          "kernelId": ""
        },
        "id": "3IHK89djnUKZ"
      },
      "outputs": [],
      "source": [
        "dataset_train = dataset_train.iloc[:, :].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "ce8f7d85-00bb-40a9-a8dd-cd04c4f46935",
          "kernelId": ""
        },
        "id": "zqeLub8TdJo3",
        "outputId": "ed8c5e99-414d-471b-82fe-2c8b276993d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13041, 2048)\n",
            "(13041, 1)\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into features and labels.\n",
        "cleandData = pd.DataFrame(dataset_train)\n",
        "images = cleandData.iloc[:, 0:2048].values\n",
        "labels = cleandData.iloc[:, 2048:2049].values\n",
        "\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "866b77b3-1e66-4cd9-929b-c0446b0f790f",
          "kernelId": ""
        },
        "id": "K11rO2zgdRx9",
        "outputId": "94c6d59a-4378-4854-858b-fc5ce6d2b755"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['can you repeat that please'],\n",
              "       ['can you repeat that please'],\n",
              "       ['can you repeat that please'],\n",
              "       ...,\n",
              "       ['i am very happy'],\n",
              "       ['i am very happy'],\n",
              "       ['i am very happy']], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "6482774a-e7e9-4102-ae6a-130de282453e",
          "kernelId": ""
        },
        "id": "srC7JJJqw4H1",
        "outputId": "64fbc6cb-064d-4925-a1ea-7883be9a7988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique words:  ['null', 'please', 'am', 'today', 'i', 'she', 'my', 'hiding', 'really', 'he', 'friend', 'happy', 'hi', 'you', 'that', 'how', 'is', 'repeat', 'can', 'something', 'appreciate', 'free', 'are', 'it', 'congratulations', 'do', 'worry', 'very', 'not', 'help']\n",
            "Number of unique words:  30\n",
            "Maximum length of any sentence in the dataset:  5\n"
          ]
        }
      ],
      "source": [
        "# Extract unique words from labels.\n",
        "characters = []\n",
        "max_length = 0\n",
        "\n",
        "for sentenc in labels:\n",
        "  splitted_text= sentenc[0].split()\n",
        "  c = 0\n",
        "  for word in splitted_text:\n",
        "    characters.append(word)\n",
        "    c = c + 1\n",
        "  if c > max_length:\n",
        "    max_length = c\n",
        "\n",
        "characters = list(set(characters))\n",
        "# characters = ['null', 'your', 'hiding', 'hair', 'that', 'water', 'me', 'you', 'repeat', 'are', 'something', 'congratulations', 'please', 'free', 'i', 'help', 'bring', 'can', 'today', 'comb', 'for']\n",
        "characters.insert(0, \"null\")\n",
        "# Unique words list from the sentence\n",
        "print(\"Unique words: \",characters)\n",
        "# Number of unique word\n",
        "print(\"Number of unique words: \",len(characters))\n",
        "# Maximum length of any sentence in the dataset\n",
        "print(\"Maximum length of any sentence in the dataset: \",max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "c36d010d-356d-4dbd-a7f6-7819a12575f1",
          "kernelId": ""
        },
        "id": "M-ue-3WCd22N",
        "outputId": "21172878-6eb4-4529-d332-a80b11f85b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['null', 'please', 'am', 'today', 'i', 'she', 'my', 'hiding', 'really', 'he', 'friend', 'happy', 'hi', 'you', 'that', 'how', 'is', 'repeat', 'can', 'something', 'appreciate', 'free', 'are', 'it', 'congratulations', 'do', 'worry', 'very', 'not', 'help']\n",
            "Check index at position 1 : please\n"
          ]
        }
      ],
      "source": [
        "# Define a method for converting string labels to numerical values.\n",
        "def text_to_num_dictionary(list_of_list, key_col=0, val_col=1):\n",
        "    value_dict = {}\n",
        "    for i,value in enumerate(list_of_list):\n",
        "      if i != 0:\n",
        "        v = {value: str(i)}\n",
        "        value_dict.update(v)\n",
        "      else:\n",
        "        v = {value: \"null\"}\n",
        "        value_dict.update(v)\n",
        "    return value_dict\n",
        "\n",
        "def num_to_text_dictionary(list_of_list, key_col=0, val_col=1):\n",
        "    value_dict = {}\n",
        "    for i,value in enumerate(list_of_list):\n",
        "      if i != 0:\n",
        "        v = {str(i):value}\n",
        "        value_dict.update(v)\n",
        "    return value_dict\n",
        "\n",
        "text_to_num_dict =  text_to_num_dictionary(characters)\n",
        "num_to_text_dict =  num_to_text_dictionary(characters)\n",
        "\n",
        "# printing original string\n",
        "print(characters)\n",
        "print(\"Check index at position 1 : \" + num_to_text_dict['1'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "826b7324-1ef2-4745-95f4-d1d459d43543",
          "kernelId": ""
        },
        "id": "Of4EoDOcKTzn",
        "outputId": "73cd4c7b-3911-4e6e-e0a4-007f0767aa9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0 16.79267 23.245064000000003 ... 11.754963 4.6680093000000005 0.0]\n",
            "[0.         0.73982057 0.62871894 ... 0.63529887 0.20789976 0.        ]\n"
          ]
        }
      ],
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "print(images[0])\n",
        "images = sc.fit_transform(images)\n",
        "print(images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "13c17258-e058-4de7-be61-66b959a2d161",
          "kernelId": ""
        },
        "id": "Ww1aJDDTrGkZ",
        "outputId": "e36f16eb-3665-4755-ea8e-316718e05b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13041, 2048)\n",
            "(13041, 1)\n",
            "(207, 63, 2048)\n",
            "(207, 1)\n"
          ]
        }
      ],
      "source": [
        "# Reshape label and image data.\n",
        "c=0\n",
        "wordArry = []\n",
        "X = []\n",
        "y = []\n",
        "for i in range(0,len(images)):\n",
        "  c = c + 1\n",
        "  wordArry.append(images[i])\n",
        "  if ( c == 63 ):\n",
        "    X.append(wordArry)\n",
        "    y.append(labels[i])\n",
        "    wordArry = []\n",
        "    c = 0\n",
        "print(np.asarray(X).shape)\n",
        "print(np.asarray(y).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "4b927e38-e9f6-46bb-ad2f-05cf22bc952d",
          "kernelId": ""
        },
        "id": "W3FRjofR-HGf",
        "outputId": "32c18ede-9567-4b57-b8ad-6ddbc67253d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(207, 5)\n",
            "(207, 63, 2048)\n"
          ]
        }
      ],
      "source": [
        "# Convert sentence words into numbers and add padding to Y.\n",
        "def convert(lst):\n",
        "    return (lst[0].split())\n",
        "\n",
        "t = []\n",
        "for data in y:\n",
        "  lst = convert(data)\n",
        "  tt = []\n",
        "  for data2 in lst:\n",
        "    if data2 in text_to_num_dict:\n",
        "      num_word = text_to_num_dict[data2]\n",
        "      tt.append(num_word)\n",
        "  t.append(tt)\n",
        "\n",
        "t = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      t, padding=\"post\"\n",
        "  )\n",
        "\n",
        "X_train = np.asarray(X, dtype=np.float32)\n",
        "y_train = np.asarray(t, dtype=np.float32)\n",
        "print (y_train.shape)\n",
        "print (X_train.shape)\n",
        "del X\n",
        "del t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "714e1f67-735a-4486-bfd2-124058e2a651",
          "kernelId": ""
        },
        "id": "rlpF8kNnKaKL"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "3fd107ef-5501-434a-845a-9f67c5f7db80",
          "kernelId": ""
        },
        "id": "e0SF8NwdzhtA"
      },
      "outputs": [],
      "source": [
        "# Part 2 - Creating the model.\n",
        "class CTCLayer(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "d931413d-9d45-46bc-b467-248aaa97815b",
          "kernelId": ""
        },
        "id": "yw3jiLLGV7Nb"
      },
      "outputs": [],
      "source": [
        "# RNN\n",
        "input = Input(shape=(63, 2048), name=\"input\")\n",
        "x = Bidirectional(LSTM(1024, activation='tanh',return_sequences=True, dropout=0.8, batch_size=32))(input)\n",
        "x = Bidirectional(LSTM(1024, activation='tanh',return_sequences=True, dropout=0.8, batch_size=32))(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "20407a2d-b290-42f3-a732-98c42ce7956f",
          "kernelId": ""
        },
        "id": "VFDFTYXHDRkh",
        "outputId": "71baf0d7-39dc-4e24-fd79-b9d60e45a0d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Continuous_EthSL_recognition_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 63, 2048)]   0           []                               \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 63, 2048)     25174016    ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 63, 2048)    25174016    ['bidirectional[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " label (InputLayer)             [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dense2 (Dense)                 (None, 63, 31)       63519       ['bidirectional_1[0][0]']        \n",
            "                                                                                                  \n",
            " ctc_loss (CTCLayer)            (None, 63, 31)       0           ['label[0][0]',                  \n",
            "                                                                  'dense2[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 50,411,551\n",
            "Trainable params: 50,411,551\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Output layer\n",
        "cc =len(characters)\n",
        "x = Dense((cc+1), activation=\"softmax\", name=\"dense2\")(x)\n",
        "\n",
        "# Input layer for the CTC\n",
        "labels = layers.Input(name=\"label\", shape=( None,), dtype=\"float32\")\n",
        "\n",
        "# Add CTC layer for calculating CTC loss at each step\n",
        "output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.models.Model(\n",
        "    inputs=[input, labels], outputs=output, name=\"Continuous_EthSL_recognition_model\"\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\"adam\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "6172806d-c1d7-4e8e-af04-e83da70ec55a",
          "kernelId": ""
        },
        "id": "RhkNMcjw6PHN",
        "outputId": "fea471fa-22d9-4105-9f8b-777bfeb99f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "5/5 [==============================] - 16s 1s/step - loss: 79.1108 - val_loss: 48.2214\n",
            "Epoch 2/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 26.1680 - val_loss: 21.7961\n",
            "Epoch 3/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 18.3400 - val_loss: 17.9786\n",
            "Epoch 4/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 17.0197 - val_loss: 16.7796\n",
            "Epoch 5/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 15.8915 - val_loss: 16.2995\n",
            "Epoch 6/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 15.2900 - val_loss: 15.3366\n",
            "Epoch 7/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 14.7979 - val_loss: 15.0136\n",
            "Epoch 8/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 14.4909 - val_loss: 15.0868\n",
            "Epoch 9/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 14.0575 - val_loss: 14.2982\n",
            "Epoch 10/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 13.7072 - val_loss: 13.4424\n",
            "Epoch 11/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 13.3633 - val_loss: 13.2871\n",
            "Epoch 12/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 13.0544 - val_loss: 12.6534\n",
            "Epoch 13/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 12.7630 - val_loss: 12.2005\n",
            "Epoch 14/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 12.5323 - val_loss: 11.8113\n",
            "Epoch 15/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 12.1986 - val_loss: 11.3164\n",
            "Epoch 16/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 12.0750 - val_loss: 11.2883\n",
            "Epoch 17/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 11.9404 - val_loss: 10.6801\n",
            "Epoch 18/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 11.5410 - val_loss: 10.7849\n",
            "Epoch 19/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 11.3948 - val_loss: 10.6593\n",
            "Epoch 20/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 11.1934 - val_loss: 10.1774\n",
            "Epoch 21/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 10.8440 - val_loss: 10.5680\n",
            "Epoch 22/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 10.8244 - val_loss: 10.1404\n",
            "Epoch 23/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 10.7930 - val_loss: 10.9105\n",
            "Epoch 24/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 10.6469 - val_loss: 10.7050\n",
            "Epoch 25/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 10.8089 - val_loss: 10.6461\n",
            "Epoch 26/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 10.6702 - val_loss: 9.4884\n",
            "Epoch 27/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 10.3534 - val_loss: 9.4633\n",
            "Epoch 28/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 10.3038 - val_loss: 9.1201\n",
            "Epoch 29/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 10.0981 - val_loss: 9.3305\n",
            "Epoch 30/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 10.0478 - val_loss: 9.1213\n",
            "Epoch 31/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 9.9575 - val_loss: 8.6371\n",
            "Epoch 32/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 9.9828 - val_loss: 9.4429\n",
            "Epoch 33/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 9.8772 - val_loss: 8.8552\n",
            "Epoch 34/500\n",
            "5/5 [==============================] - 3s 700ms/step - loss: 9.7085 - val_loss: 8.7484\n",
            "Epoch 35/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 9.4758 - val_loss: 8.4475\n",
            "Epoch 36/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 9.4006 - val_loss: 8.3463\n",
            "Epoch 37/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 9.3088 - val_loss: 8.7532\n",
            "Epoch 38/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 9.2768 - val_loss: 8.0963\n",
            "Epoch 39/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 9.1348 - val_loss: 8.2237\n",
            "Epoch 40/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 9.6906 - val_loss: 8.6396\n",
            "Epoch 41/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 9.5391 - val_loss: 9.6579\n",
            "Epoch 42/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 9.7365 - val_loss: 8.9241\n",
            "Epoch 43/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 9.0936 - val_loss: 8.5352\n",
            "Epoch 44/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 9.0938 - val_loss: 7.9902\n",
            "Epoch 45/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 8.9016 - val_loss: 8.3285\n",
            "Epoch 46/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 8.5923 - val_loss: 8.4682\n",
            "Epoch 47/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 8.6073 - val_loss: 8.0244\n",
            "Epoch 48/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 8.2949 - val_loss: 7.8504\n",
            "Epoch 49/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 8.1782 - val_loss: 8.0233\n",
            "Epoch 50/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 7.9926 - val_loss: 8.4603\n",
            "Epoch 51/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 8.1522 - val_loss: 8.2109\n",
            "Epoch 52/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 8.1140 - val_loss: 8.0615\n",
            "Epoch 53/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 7.9519 - val_loss: 9.0430\n",
            "Epoch 54/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 8.0675 - val_loss: 8.1955\n",
            "Epoch 55/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 7.6145 - val_loss: 7.6480\n",
            "Epoch 56/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 7.9488 - val_loss: 8.1854\n",
            "Epoch 57/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 8.0146 - val_loss: 7.7519\n",
            "Epoch 58/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 7.7423 - val_loss: 9.1283\n",
            "Epoch 59/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 7.7690 - val_loss: 8.1205\n",
            "Epoch 60/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 7.9624 - val_loss: 8.1829\n",
            "Epoch 61/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 7.5762 - val_loss: 7.6971\n",
            "Epoch 62/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 7.5003 - val_loss: 7.8005\n",
            "Epoch 63/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 7.3171 - val_loss: 7.6022\n",
            "Epoch 64/500\n",
            "5/5 [==============================] - 3s 715ms/step - loss: 7.2791 - val_loss: 8.1937\n",
            "Epoch 65/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 7.1068 - val_loss: 7.5879\n",
            "Epoch 66/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 6.8871 - val_loss: 7.3546\n",
            "Epoch 67/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 6.9935 - val_loss: 8.1107\n",
            "Epoch 68/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 7.2989 - val_loss: 7.3648\n",
            "Epoch 69/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 7.4342 - val_loss: 8.2463\n",
            "Epoch 70/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 7.4778 - val_loss: 8.0324\n",
            "Epoch 71/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 7.0293 - val_loss: 7.3952\n",
            "Epoch 72/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 6.8174 - val_loss: 6.7962\n",
            "Epoch 73/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 6.5557 - val_loss: 7.1737\n",
            "Epoch 74/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 6.3742 - val_loss: 6.4329\n",
            "Epoch 75/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 6.1350 - val_loss: 6.5679\n",
            "Epoch 76/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 6.1695 - val_loss: 6.8575\n",
            "Epoch 77/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 6.1725 - val_loss: 7.2406\n",
            "Epoch 78/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 6.4474 - val_loss: 8.0622\n",
            "Epoch 79/500\n",
            "5/5 [==============================] - 4s 720ms/step - loss: 6.5222 - val_loss: 7.5810\n",
            "Epoch 80/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 6.2416 - val_loss: 7.0563\n",
            "Epoch 81/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 6.2612 - val_loss: 7.4132\n",
            "Epoch 82/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 6.0212 - val_loss: 6.9962\n",
            "Epoch 83/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 6.0600 - val_loss: 7.9461\n",
            "Epoch 84/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 5.7440 - val_loss: 7.2237\n",
            "Epoch 85/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 5.4104 - val_loss: 8.2540\n",
            "Epoch 86/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 5.9921 - val_loss: 7.5031\n",
            "Epoch 87/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 5.9753 - val_loss: 7.0313\n",
            "Epoch 88/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 5.2680 - val_loss: 7.4852\n",
            "Epoch 89/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 5.2798 - val_loss: 8.0170\n",
            "Epoch 90/500\n",
            "5/5 [==============================] - 3s 714ms/step - loss: 5.4795 - val_loss: 8.3644\n",
            "Epoch 91/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 5.7759 - val_loss: 6.0875\n",
            "Epoch 92/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 5.6705 - val_loss: 7.2818\n",
            "Epoch 93/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 6.0776 - val_loss: 12.4919\n",
            "Epoch 94/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 6.1906 - val_loss: 6.9644\n",
            "Epoch 95/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 5.2780 - val_loss: 7.2620\n",
            "Epoch 96/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 5.3583 - val_loss: 7.2454\n",
            "Epoch 97/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 4.9759 - val_loss: 8.5023\n",
            "Epoch 98/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 4.9533 - val_loss: 6.9243\n",
            "Epoch 99/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 4.7740 - val_loss: 6.8394\n",
            "Epoch 100/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 4.3489 - val_loss: 6.8888\n",
            "Epoch 101/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 4.4275 - val_loss: 7.0616\n",
            "Epoch 102/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 4.4252 - val_loss: 7.2587\n",
            "Epoch 103/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 4.5656 - val_loss: 6.3938\n",
            "Epoch 104/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 4.6030 - val_loss: 6.7452\n",
            "Epoch 105/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 4.5230 - val_loss: 5.6806\n",
            "Epoch 106/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 4.4220 - val_loss: 5.6721\n",
            "Epoch 107/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 4.6205 - val_loss: 7.1773\n",
            "Epoch 108/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 4.2099 - val_loss: 6.6184\n",
            "Epoch 109/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 4.2436 - val_loss: 6.8648\n",
            "Epoch 110/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 4.0254 - val_loss: 7.4999\n",
            "Epoch 111/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 3.9068 - val_loss: 7.7690\n",
            "Epoch 112/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 4.5525 - val_loss: 7.9875\n",
            "Epoch 113/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 5.2272 - val_loss: 8.1558\n",
            "Epoch 114/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 4.6347 - val_loss: 9.4309\n",
            "Epoch 115/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 4.8792 - val_loss: 7.0649\n",
            "Epoch 116/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 4.2735 - val_loss: 6.8035\n",
            "Epoch 117/500\n",
            "5/5 [==============================] - 3s 700ms/step - loss: 3.9930 - val_loss: 6.4893\n",
            "Epoch 118/500\n",
            "5/5 [==============================] - 3s 700ms/step - loss: 4.0318 - val_loss: 6.0745\n",
            "Epoch 119/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 3.6490 - val_loss: 5.6793\n",
            "Epoch 120/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 3.3232 - val_loss: 7.1892\n",
            "Epoch 121/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 3.3307 - val_loss: 6.3831\n",
            "Epoch 122/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 3.2261 - val_loss: 8.5643\n",
            "Epoch 123/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 3.2116 - val_loss: 5.6281\n",
            "Epoch 124/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 3.6013 - val_loss: 7.7038\n",
            "Epoch 125/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 3.6442 - val_loss: 5.4068\n",
            "Epoch 126/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 3.3743 - val_loss: 5.7495\n",
            "Epoch 127/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 3.0230 - val_loss: 6.0256\n",
            "Epoch 128/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 3.1031 - val_loss: 6.9304\n",
            "Epoch 129/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 3.4099 - val_loss: 5.9635\n",
            "Epoch 130/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 3.5502 - val_loss: 7.0427\n",
            "Epoch 131/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 2.9626 - val_loss: 7.8482\n",
            "Epoch 132/500\n",
            "5/5 [==============================] - 3s 700ms/step - loss: 3.0571 - val_loss: 5.8780\n",
            "Epoch 133/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 2.8619 - val_loss: 5.6006\n",
            "Epoch 134/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 2.7922 - val_loss: 7.4763\n",
            "Epoch 135/500\n",
            "5/5 [==============================] - 3s 700ms/step - loss: 2.9715 - val_loss: 5.6701\n",
            "Epoch 136/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 3.2564 - val_loss: 8.5803\n",
            "Epoch 137/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 3.0423 - val_loss: 7.2079\n",
            "Epoch 138/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 3.5299 - val_loss: 8.0925\n",
            "Epoch 139/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 3.5353 - val_loss: 9.4420\n",
            "Epoch 140/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 3.6780 - val_loss: 7.3897\n",
            "Epoch 141/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 3.3341 - val_loss: 5.2555\n",
            "Epoch 142/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 2.7669 - val_loss: 6.9860\n",
            "Epoch 143/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 2.8658 - val_loss: 6.4463\n",
            "Epoch 144/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 2.7052 - val_loss: 5.7751\n",
            "Epoch 145/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 2.8124 - val_loss: 7.6532\n",
            "Epoch 146/500\n",
            "5/5 [==============================] - 3s 697ms/step - loss: 2.9164 - val_loss: 8.8789\n",
            "Epoch 147/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 2.8308 - val_loss: 6.3817\n",
            "Epoch 148/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 2.9453 - val_loss: 6.7013\n",
            "Epoch 149/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 2.5914 - val_loss: 6.1556\n",
            "Epoch 150/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 2.4460 - val_loss: 6.6906\n",
            "Epoch 151/500\n",
            "5/5 [==============================] - 3s 699ms/step - loss: 1.9867 - val_loss: 5.5780\n",
            "Epoch 152/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 1.7926 - val_loss: 6.7584\n",
            "Epoch 153/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 1.8214 - val_loss: 4.9774\n",
            "Epoch 154/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.7159 - val_loss: 6.2877\n",
            "Epoch 155/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 2.0234 - val_loss: 4.8484\n",
            "Epoch 156/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 2.3979 - val_loss: 7.0025\n",
            "Epoch 157/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 2.3483 - val_loss: 5.8186\n",
            "Epoch 158/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 3.1576 - val_loss: 6.5494\n",
            "Epoch 159/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 2.3906 - val_loss: 7.8630\n",
            "Epoch 160/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 2.1889 - val_loss: 6.8847\n",
            "Epoch 161/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 2.3467 - val_loss: 7.3557\n",
            "Epoch 162/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 2.1584 - val_loss: 6.5392\n",
            "Epoch 163/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 2.2839 - val_loss: 6.7565\n",
            "Epoch 164/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.9844 - val_loss: 6.7467\n",
            "Epoch 165/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.8600 - val_loss: 5.5996\n",
            "Epoch 166/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.8884 - val_loss: 7.5225\n",
            "Epoch 167/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.5789 - val_loss: 5.8300\n",
            "Epoch 168/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.8163 - val_loss: 6.6268\n",
            "Epoch 169/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 2.0153 - val_loss: 6.7958\n",
            "Epoch 170/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 2.0938 - val_loss: 6.5208\n",
            "Epoch 171/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 2.1578 - val_loss: 7.2182\n",
            "Epoch 172/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.9363 - val_loss: 5.5723\n",
            "Epoch 173/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.6458 - val_loss: 7.3796\n",
            "Epoch 174/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.9175 - val_loss: 4.2259\n",
            "Epoch 175/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 1.3929 - val_loss: 6.4068\n",
            "Epoch 176/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 1.7842 - val_loss: 4.1981\n",
            "Epoch 177/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.6284 - val_loss: 8.9183\n",
            "Epoch 178/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.3930 - val_loss: 4.8881\n",
            "Epoch 179/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 1.6187 - val_loss: 5.7905\n",
            "Epoch 180/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 1.5245 - val_loss: 5.7843\n",
            "Epoch 181/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 1.4516 - val_loss: 5.7029\n",
            "Epoch 182/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.3897 - val_loss: 7.9337\n",
            "Epoch 183/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 1.3159 - val_loss: 6.6261\n",
            "Epoch 184/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.6733 - val_loss: 6.4316\n",
            "Epoch 185/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 1.2664 - val_loss: 8.3519\n",
            "Epoch 186/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 1.2359 - val_loss: 4.3919\n",
            "Epoch 187/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.6660 - val_loss: 8.8463\n",
            "Epoch 188/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.5943 - val_loss: 6.5912\n",
            "Epoch 189/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 1.4084 - val_loss: 5.1055\n",
            "Epoch 190/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 1.2823 - val_loss: 8.0653\n",
            "Epoch 191/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.2960 - val_loss: 5.4917\n",
            "Epoch 192/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.0824 - val_loss: 5.7165\n",
            "Epoch 193/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.8844 - val_loss: 5.2058\n",
            "Epoch 194/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.2769 - val_loss: 4.9164\n",
            "Epoch 195/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 1.3368 - val_loss: 6.0653\n",
            "Epoch 196/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.3351 - val_loss: 8.5936\n",
            "Epoch 197/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.5815 - val_loss: 4.2066\n",
            "Epoch 198/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.6048 - val_loss: 8.2185\n",
            "Epoch 199/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.4481 - val_loss: 6.2623\n",
            "Epoch 200/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.5844 - val_loss: 7.0598\n",
            "Epoch 201/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.9526 - val_loss: 6.9789\n",
            "Epoch 202/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 2.1692 - val_loss: 8.1650\n",
            "Epoch 203/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.6280 - val_loss: 8.1396\n",
            "Epoch 204/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.7520 - val_loss: 7.4113\n",
            "Epoch 205/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.3117 - val_loss: 7.2487\n",
            "Epoch 206/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.5489 - val_loss: 6.6092\n",
            "Epoch 207/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.0157 - val_loss: 5.8962\n",
            "Epoch 208/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.3309 - val_loss: 5.7864\n",
            "Epoch 209/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 1.4234 - val_loss: 8.1987\n",
            "Epoch 210/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.0566 - val_loss: 8.5562\n",
            "Epoch 211/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.6718 - val_loss: 9.0832\n",
            "Epoch 212/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.3243 - val_loss: 7.6395\n",
            "Epoch 213/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.1474 - val_loss: 10.7896\n",
            "Epoch 214/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.9381 - val_loss: 6.4970\n",
            "Epoch 215/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.9955 - val_loss: 5.9640\n",
            "Epoch 216/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 1.1582 - val_loss: 8.2188\n",
            "Epoch 217/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.8842 - val_loss: 6.7456\n",
            "Epoch 218/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.6376 - val_loss: 6.7548\n",
            "Epoch 219/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.6039 - val_loss: 5.8305\n",
            "Epoch 220/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.5010 - val_loss: 5.7789\n",
            "Epoch 221/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.5843 - val_loss: 5.7562\n",
            "Epoch 222/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 0.8019 - val_loss: 6.7554\n",
            "Epoch 223/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.7516 - val_loss: 6.5802\n",
            "Epoch 224/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 0.8780 - val_loss: 7.3184\n",
            "Epoch 225/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.7312 - val_loss: 7.3482\n",
            "Epoch 226/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.9356 - val_loss: 6.1938\n",
            "Epoch 227/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.7049 - val_loss: 7.8521\n",
            "Epoch 228/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.7948 - val_loss: 7.2509\n",
            "Epoch 229/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.7872 - val_loss: 6.2989\n",
            "Epoch 230/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.7555 - val_loss: 6.4755\n",
            "Epoch 231/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.9010 - val_loss: 7.1606\n",
            "Epoch 232/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.8782 - val_loss: 11.7585\n",
            "Epoch 233/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.3518 - val_loss: 12.3122\n",
            "Epoch 234/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.2730 - val_loss: 7.3076\n",
            "Epoch 235/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.2620 - val_loss: 7.5780\n",
            "Epoch 236/500\n",
            "5/5 [==============================] - 3s 699ms/step - loss: 0.7939 - val_loss: 5.1076\n",
            "Epoch 237/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 1.5087 - val_loss: 8.6874\n",
            "Epoch 238/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.3942 - val_loss: 6.8976\n",
            "Epoch 239/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.0132 - val_loss: 9.7605\n",
            "Epoch 240/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 1.1814 - val_loss: 6.1998\n",
            "Epoch 241/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.9080 - val_loss: 6.7970\n",
            "Epoch 242/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 0.8149 - val_loss: 8.2146\n",
            "Epoch 243/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.7150 - val_loss: 6.8930\n",
            "Epoch 244/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.7362 - val_loss: 6.0309\n",
            "Epoch 245/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.7045 - val_loss: 7.0270\n",
            "Epoch 246/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.5265 - val_loss: 8.6721\n",
            "Epoch 247/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.7483 - val_loss: 7.7008\n",
            "Epoch 248/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.7200 - val_loss: 6.4766\n",
            "Epoch 249/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.6921 - val_loss: 8.5424\n",
            "Epoch 250/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 0.4359 - val_loss: 8.0598\n",
            "Epoch 251/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 0.5657 - val_loss: 8.9237\n",
            "Epoch 252/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.8740 - val_loss: 8.4848\n",
            "Epoch 253/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 0.9120 - val_loss: 8.2397\n",
            "Epoch 254/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.7674 - val_loss: 7.2551\n",
            "Epoch 255/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.6382 - val_loss: 8.3966\n",
            "Epoch 256/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.7341 - val_loss: 7.0669\n",
            "Epoch 257/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.8869 - val_loss: 7.0729\n",
            "Epoch 258/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.6702 - val_loss: 5.4700\n",
            "Epoch 259/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.5681 - val_loss: 5.9740\n",
            "Epoch 260/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.5291 - val_loss: 7.5021\n",
            "Epoch 261/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.5255 - val_loss: 5.9743\n",
            "Epoch 262/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.4590 - val_loss: 7.4786\n",
            "Epoch 263/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.4316 - val_loss: 6.3756\n",
            "Epoch 264/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.4796 - val_loss: 4.9907\n",
            "Epoch 265/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.5907 - val_loss: 6.8791\n",
            "Epoch 266/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.4021 - val_loss: 10.7562\n",
            "Epoch 267/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.9399 - val_loss: 5.7020\n",
            "Epoch 268/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 0.8923 - val_loss: 6.4295\n",
            "Epoch 269/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.7083 - val_loss: 7.5761\n",
            "Epoch 270/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 1.6420 - val_loss: 9.0748\n",
            "Epoch 271/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 1.4847 - val_loss: 7.9206\n",
            "Epoch 272/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.5355 - val_loss: 8.6168\n",
            "Epoch 273/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 1.3082 - val_loss: 5.7650\n",
            "Epoch 274/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.1444 - val_loss: 5.8238\n",
            "Epoch 275/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.9807 - val_loss: 5.6949\n",
            "Epoch 276/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.7225 - val_loss: 7.3260\n",
            "Epoch 277/500\n",
            "5/5 [==============================] - 3s 715ms/step - loss: 1.0831 - val_loss: 4.1720\n",
            "Epoch 278/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.6308 - val_loss: 4.3835\n",
            "Epoch 279/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.5836 - val_loss: 5.1886\n",
            "Epoch 280/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.3843 - val_loss: 6.3518\n",
            "Epoch 281/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.8139 - val_loss: 6.9559\n",
            "Epoch 282/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.6226 - val_loss: 6.1329\n",
            "Epoch 283/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.8916 - val_loss: 6.9885\n",
            "Epoch 284/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.7278 - val_loss: 7.8538\n",
            "Epoch 285/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.6643 - val_loss: 6.6628\n",
            "Epoch 286/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.5024 - val_loss: 8.1537\n",
            "Epoch 287/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.0654 - val_loss: 5.8639\n",
            "Epoch 288/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.2000 - val_loss: 8.8488\n",
            "Epoch 289/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 2.7648 - val_loss: 11.6007\n",
            "Epoch 290/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 2.5761 - val_loss: 14.4738\n",
            "Epoch 291/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.5572 - val_loss: 8.6179\n",
            "Epoch 292/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.4930 - val_loss: 8.3016\n",
            "Epoch 293/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.9632 - val_loss: 8.3981\n",
            "Epoch 294/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.8215 - val_loss: 6.4325\n",
            "Epoch 295/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.2052 - val_loss: 4.0028\n",
            "Epoch 296/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.7017 - val_loss: 5.7751\n",
            "Epoch 297/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.6731 - val_loss: 8.4061\n",
            "Epoch 298/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.6598 - val_loss: 6.4391\n",
            "Epoch 299/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.3937 - val_loss: 6.1591\n",
            "Epoch 300/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 0.3910 - val_loss: 6.1622\n",
            "Epoch 301/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.4250 - val_loss: 6.4506\n",
            "Epoch 302/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.4983 - val_loss: 7.3182\n",
            "Epoch 303/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.9792 - val_loss: 7.9370\n",
            "Epoch 304/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.9725 - val_loss: 7.4271\n",
            "Epoch 305/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.7740 - val_loss: 6.7312\n",
            "Epoch 306/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 1.2057 - val_loss: 8.7289\n",
            "Epoch 307/500\n",
            "5/5 [==============================] - 3s 714ms/step - loss: 1.8413 - val_loss: 8.9797\n",
            "Epoch 308/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.1430 - val_loss: 7.3674\n",
            "Epoch 309/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.3129 - val_loss: 5.7823\n",
            "Epoch 310/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 1.3947 - val_loss: 7.5687\n",
            "Epoch 311/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.0607 - val_loss: 7.3250\n",
            "Epoch 312/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.9371 - val_loss: 7.8345\n",
            "Epoch 313/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 1.0481 - val_loss: 6.7605\n",
            "Epoch 314/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.5969 - val_loss: 6.3199\n",
            "Epoch 315/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.6956 - val_loss: 5.6201\n",
            "Epoch 316/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.6003 - val_loss: 5.8953\n",
            "Epoch 317/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.3978 - val_loss: 6.5671\n",
            "Epoch 318/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.5237 - val_loss: 5.5097\n",
            "Epoch 319/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.3407 - val_loss: 5.8850\n",
            "Epoch 320/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.2914 - val_loss: 5.3847\n",
            "Epoch 321/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.4559 - val_loss: 6.1870\n",
            "Epoch 322/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.2852 - val_loss: 6.9116\n",
            "Epoch 323/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2440 - val_loss: 7.2401\n",
            "Epoch 324/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.3497 - val_loss: 5.5061\n",
            "Epoch 325/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.4571 - val_loss: 5.7472\n",
            "Epoch 326/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2525 - val_loss: 6.5766\n",
            "Epoch 327/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.3503 - val_loss: 7.0590\n",
            "Epoch 328/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.1838 - val_loss: 7.6124\n",
            "Epoch 329/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.1956 - val_loss: 7.5859\n",
            "Epoch 330/500\n",
            "5/5 [==============================] - 3s 716ms/step - loss: 0.2097 - val_loss: 7.2610\n",
            "Epoch 331/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.2974 - val_loss: 6.9858\n",
            "Epoch 332/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.2483 - val_loss: 6.2370\n",
            "Epoch 333/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2410 - val_loss: 6.4997\n",
            "Epoch 334/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.3041 - val_loss: 6.4052\n",
            "Epoch 335/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.6608 - val_loss: 5.2381\n",
            "Epoch 336/500\n",
            "5/5 [==============================] - 3s 715ms/step - loss: 0.6355 - val_loss: 5.6709\n",
            "Epoch 337/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.7239 - val_loss: 6.8158\n",
            "Epoch 338/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.3481 - val_loss: 6.9676\n",
            "Epoch 339/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.3801 - val_loss: 7.1323\n",
            "Epoch 340/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.3849 - val_loss: 7.4435\n",
            "Epoch 341/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2818 - val_loss: 6.6841\n",
            "Epoch 342/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.2918 - val_loss: 6.5568\n",
            "Epoch 343/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.2205 - val_loss: 6.3027\n",
            "Epoch 344/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.2323 - val_loss: 6.2467\n",
            "Epoch 345/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.3470 - val_loss: 6.8187\n",
            "Epoch 346/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.4140 - val_loss: 6.7292\n",
            "Epoch 347/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 0.2619 - val_loss: 5.5759\n",
            "Epoch 348/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.2615 - val_loss: 5.8343\n",
            "Epoch 349/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 0.3557 - val_loss: 6.0851\n",
            "Epoch 350/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.2775 - val_loss: 6.3210\n",
            "Epoch 351/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.3854 - val_loss: 8.0315\n",
            "Epoch 352/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.3866 - val_loss: 6.3758\n",
            "Epoch 353/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.3721 - val_loss: 6.4718\n",
            "Epoch 354/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 0.4276 - val_loss: 6.1522\n",
            "Epoch 355/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.6430 - val_loss: 5.0894\n",
            "Epoch 356/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.6901 - val_loss: 8.6610\n",
            "Epoch 357/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.6749 - val_loss: 6.9632\n",
            "Epoch 358/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 1.1336 - val_loss: 8.5980\n",
            "Epoch 359/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.2553 - val_loss: 4.7307\n",
            "Epoch 360/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 1.3961 - val_loss: 3.1193\n",
            "Epoch 361/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.0038 - val_loss: 5.8482\n",
            "Epoch 362/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.6828 - val_loss: 5.0246\n",
            "Epoch 363/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 0.5781 - val_loss: 7.8483\n",
            "Epoch 364/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.7788 - val_loss: 6.0074\n",
            "Epoch 365/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.6158 - val_loss: 6.4993\n",
            "Epoch 366/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.6770 - val_loss: 7.0604\n",
            "Epoch 367/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.6362 - val_loss: 5.7367\n",
            "Epoch 368/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.4045 - val_loss: 5.3305\n",
            "Epoch 369/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.6245 - val_loss: 6.5053\n",
            "Epoch 370/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.7770 - val_loss: 6.0443\n",
            "Epoch 371/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.6859 - val_loss: 6.0329\n",
            "Epoch 372/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.5404 - val_loss: 6.4811\n",
            "Epoch 373/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.3529 - val_loss: 4.9191\n",
            "Epoch 374/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.5184 - val_loss: 5.7902\n",
            "Epoch 375/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.6270 - val_loss: 7.4532\n",
            "Epoch 376/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.6525 - val_loss: 8.9885\n",
            "Epoch 377/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.7655 - val_loss: 7.0038\n",
            "Epoch 378/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.4190 - val_loss: 5.2584\n",
            "Epoch 379/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.3249 - val_loss: 4.1561\n",
            "Epoch 380/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.3934 - val_loss: 7.2717\n",
            "Epoch 381/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.4208 - val_loss: 2.4787\n",
            "Epoch 382/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.8272 - val_loss: 3.8971\n",
            "Epoch 383/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.7280 - val_loss: 4.9547\n",
            "Epoch 384/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.8124 - val_loss: 10.4602\n",
            "Epoch 385/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 2.2257 - val_loss: 11.1662\n",
            "Epoch 386/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 1.0767 - val_loss: 6.8220\n",
            "Epoch 387/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.8753 - val_loss: 8.5321\n",
            "Epoch 388/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.8756 - val_loss: 6.6689\n",
            "Epoch 389/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.8047 - val_loss: 6.0743\n",
            "Epoch 390/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.6676 - val_loss: 5.5476\n",
            "Epoch 391/500\n",
            "5/5 [==============================] - 3s 702ms/step - loss: 0.5066 - val_loss: 7.2204\n",
            "Epoch 392/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.4249 - val_loss: 6.3979\n",
            "Epoch 393/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.4812 - val_loss: 7.9910\n",
            "Epoch 394/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.2780 - val_loss: 9.4725\n",
            "Epoch 395/500\n",
            "5/5 [==============================] - 3s 714ms/step - loss: 0.4219 - val_loss: 8.0971\n",
            "Epoch 396/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.2131 - val_loss: 8.4811\n",
            "Epoch 397/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.3350 - val_loss: 7.2504\n",
            "Epoch 398/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2415 - val_loss: 5.4361\n",
            "Epoch 399/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.1796 - val_loss: 5.0183\n",
            "Epoch 400/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.4312 - val_loss: 7.0252\n",
            "Epoch 401/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.2197 - val_loss: 6.9561\n",
            "Epoch 402/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.4554 - val_loss: 6.9722\n",
            "Epoch 403/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.2824 - val_loss: 6.8397\n",
            "Epoch 404/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.2282 - val_loss: 6.5243\n",
            "Epoch 405/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2712 - val_loss: 7.0934\n",
            "Epoch 406/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.2359 - val_loss: 8.1216\n",
            "Epoch 407/500\n",
            "5/5 [==============================] - 4s 726ms/step - loss: 0.1617 - val_loss: 6.7486\n",
            "Epoch 408/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.1592 - val_loss: 6.4456\n",
            "Epoch 409/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.1522 - val_loss: 6.3749\n",
            "Epoch 410/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2065 - val_loss: 6.8843\n",
            "Epoch 411/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.1164 - val_loss: 6.9700\n",
            "Epoch 412/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.4137 - val_loss: 4.7056\n",
            "Epoch 413/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.1774 - val_loss: 5.1559\n",
            "Epoch 414/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.3746 - val_loss: 6.5496\n",
            "Epoch 415/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.1921 - val_loss: 7.4849\n",
            "Epoch 416/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.2461 - val_loss: 7.8558\n",
            "Epoch 417/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2890 - val_loss: 7.8197\n",
            "Epoch 418/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2056 - val_loss: 7.5098\n",
            "Epoch 419/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.2057 - val_loss: 8.8806\n",
            "Epoch 420/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 0.4921 - val_loss: 6.4907\n",
            "Epoch 421/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.3001 - val_loss: 6.4320\n",
            "Epoch 422/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.2220 - val_loss: 5.3812\n",
            "Epoch 423/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.1745 - val_loss: 4.7930\n",
            "Epoch 424/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.4903 - val_loss: 4.6648\n",
            "Epoch 425/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.3972 - val_loss: 5.4281\n",
            "Epoch 426/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.1230 - val_loss: 7.1524\n",
            "Epoch 427/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.1478 - val_loss: 8.1812\n",
            "Epoch 428/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.2896 - val_loss: 6.3613\n",
            "Epoch 429/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2150 - val_loss: 6.0464\n",
            "Epoch 430/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2305 - val_loss: 6.9551\n",
            "Epoch 431/500\n",
            "5/5 [==============================] - 3s 714ms/step - loss: 0.2221 - val_loss: 5.9452\n",
            "Epoch 432/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.4019 - val_loss: 6.0594\n",
            "Epoch 433/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2390 - val_loss: 5.9039\n",
            "Epoch 434/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.6283 - val_loss: 6.4741\n",
            "Epoch 435/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 1.2066 - val_loss: 8.8429\n",
            "Epoch 436/500\n",
            "5/5 [==============================] - 3s 713ms/step - loss: 1.4415 - val_loss: 10.5353\n",
            "Epoch 437/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 1.8161 - val_loss: 4.7100\n",
            "Epoch 438/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 2.4115 - val_loss: 8.6758\n",
            "Epoch 439/500\n",
            "5/5 [==============================] - 3s 701ms/step - loss: 1.9369 - val_loss: 8.1895\n",
            "Epoch 440/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 1.1809 - val_loss: 5.4212\n",
            "Epoch 441/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.8512 - val_loss: 5.4913\n",
            "Epoch 442/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.6051 - val_loss: 6.5266\n",
            "Epoch 443/500\n",
            "5/5 [==============================] - 3s 714ms/step - loss: 0.4210 - val_loss: 8.4249\n",
            "Epoch 444/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.3329 - val_loss: 8.8709\n",
            "Epoch 445/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.3087 - val_loss: 9.2147\n",
            "Epoch 446/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.2144 - val_loss: 8.2369\n",
            "Epoch 447/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.3256 - val_loss: 7.7032\n",
            "Epoch 448/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2033 - val_loss: 7.4237\n",
            "Epoch 449/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2276 - val_loss: 7.4788\n",
            "Epoch 450/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.1904 - val_loss: 8.3443\n",
            "Epoch 451/500\n",
            "5/5 [==============================] - 3s 715ms/step - loss: 0.1731 - val_loss: 8.4001\n",
            "Epoch 452/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.1378 - val_loss: 7.2541\n",
            "Epoch 453/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.1192 - val_loss: 6.9488\n",
            "Epoch 454/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.1680 - val_loss: 6.8726\n",
            "Epoch 455/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.1842 - val_loss: 6.7288\n",
            "Epoch 456/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.1447 - val_loss: 6.5727\n",
            "Epoch 457/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.1994 - val_loss: 6.8454\n",
            "Epoch 458/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.1004 - val_loss: 7.1305\n",
            "Epoch 459/500\n",
            "5/5 [==============================] - 3s 703ms/step - loss: 0.2175 - val_loss: 6.8384\n",
            "Epoch 460/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2063 - val_loss: 6.9771\n",
            "Epoch 461/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.1690 - val_loss: 5.9014\n",
            "Epoch 462/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.2132 - val_loss: 5.5996\n",
            "Epoch 463/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.1944 - val_loss: 5.6113\n",
            "Epoch 464/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.0754 - val_loss: 6.4095\n",
            "Epoch 465/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.1027 - val_loss: 6.4292\n",
            "Epoch 466/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.2310 - val_loss: 7.1297\n",
            "Epoch 467/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.4116 - val_loss: 6.8229\n",
            "Epoch 468/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.4436 - val_loss: 6.7610\n",
            "Epoch 469/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.3906 - val_loss: 8.9766\n",
            "Epoch 470/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.4633 - val_loss: 10.0101\n",
            "Epoch 471/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.4208 - val_loss: 8.6520\n",
            "Epoch 472/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.3161 - val_loss: 6.4448\n",
            "Epoch 473/500\n",
            "5/5 [==============================] - 3s 714ms/step - loss: 1.0795 - val_loss: 9.8984\n",
            "Epoch 474/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.5560 - val_loss: 12.0293\n",
            "Epoch 475/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.6544 - val_loss: 6.9628\n",
            "Epoch 476/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.6728 - val_loss: 6.6221\n",
            "Epoch 477/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.4580 - val_loss: 8.0305\n",
            "Epoch 478/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.2897 - val_loss: 7.7401\n",
            "Epoch 479/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2421 - val_loss: 7.3785\n",
            "Epoch 480/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.1683 - val_loss: 7.7301\n",
            "Epoch 481/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2217 - val_loss: 7.2315\n",
            "Epoch 482/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.1670 - val_loss: 8.0508\n",
            "Epoch 483/500\n",
            "5/5 [==============================] - 3s 710ms/step - loss: 0.1915 - val_loss: 8.0740\n",
            "Epoch 484/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.1426 - val_loss: 8.0131\n",
            "Epoch 485/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.1877 - val_loss: 8.7635\n",
            "Epoch 486/500\n",
            "5/5 [==============================] - 3s 704ms/step - loss: 0.1612 - val_loss: 8.4014\n",
            "Epoch 487/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.1414 - val_loss: 8.2145\n",
            "Epoch 488/500\n",
            "5/5 [==============================] - 3s 706ms/step - loss: 0.2153 - val_loss: 7.3672\n",
            "Epoch 489/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.1383 - val_loss: 6.1602\n",
            "Epoch 490/500\n",
            "5/5 [==============================] - 3s 705ms/step - loss: 0.2056 - val_loss: 6.2507\n",
            "Epoch 491/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.2531 - val_loss: 5.9102\n",
            "Epoch 492/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.1457 - val_loss: 8.0625\n",
            "Epoch 493/500\n",
            "5/5 [==============================] - 3s 709ms/step - loss: 0.1262 - val_loss: 9.7073\n",
            "Epoch 494/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.1492 - val_loss: 9.9638\n",
            "Epoch 495/500\n",
            "5/5 [==============================] - 3s 712ms/step - loss: 0.2854 - val_loss: 7.4525\n",
            "Epoch 496/500\n",
            "5/5 [==============================] - 3s 711ms/step - loss: 0.1690 - val_loss: 7.5203\n",
            "Epoch 497/500\n",
            "5/5 [==============================] - 3s 708ms/step - loss: 0.1616 - val_loss: 9.1483\n",
            "Epoch 498/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.2289 - val_loss: 9.3655\n",
            "Epoch 499/500\n",
            "5/5 [==============================] - 3s 715ms/step - loss: 0.5302 - val_loss: 8.7876\n",
            "Epoch 500/500\n",
            "5/5 [==============================] - 3s 707ms/step - loss: 0.1968 - val_loss: 7.8288\n"
          ]
        }
      ],
      "source": [
        "early_stopping_patience = 10000\n",
        "# Add early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "history = model.fit((X_train, y_train), epochs=500, validation_split = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "c28dc642-660c-447f-a0a2-bf6b6dffd2c0",
          "kernelId": ""
        },
        "id": "5JwOJ7jtkuar",
        "outputId": "fd383ab8-e94c-49ec-b866-49a54507123b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Prediction_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 63, 2048)]        0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 63, 2048)         25174016  \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 63, 2048)         25174016  \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense2 (Dense)              (None, 63, 31)            63519     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,411,551\n",
            "Trainable params: 50,411,551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Get the prediction model by extracting layers till the output layer\n",
        "prediction_model = Model(\n",
        "    model.get_layer(name=\"input\").input, model.get_layer(name=\"dense2\").output, name=\"Prediction_model\"\n",
        ")\n",
        "prediction_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "1ca64222-65a7-413d-9e5e-35134cc9cc20",
          "kernelId": ""
        },
        "id": "h-Adu9oqKIxX",
        "outputId": "a3f630b6-0b61-497d-d1e0-89fb8b1ab4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------\n",
            "Expected output :  ['congratulations']\n",
            "Predicted output:  ['i', 'am', 'very', 'happy']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Predicted output:  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['congratulations']\n",
            "Predicted output:  ['i', 'am', 'worry']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'really', 'appreciate', 'it']\n",
            "Predicted output:  ['i', 'really', 'appreciate', 'it']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['he', 'she', 'is', 'my', 'friend']\n",
            "Predicted output:  ['he', 'she', 'is', 'my', 'friend']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['hi', 'how', 'are', 'you']\n",
            "Predicted output:  ['hi', 'how', 'are', 'you']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Predicted output:  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['congratulations']\n",
            "Predicted output:  ['i', 'am', 'very', 'happy']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['do', 'not', 'worry']\n",
            "Predicted output:  ['do', 'not', 'worry']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['he', 'she', 'is', 'my', 'friend']\n",
            "Predicted output:  ['do', 'not', 'worry', 'f', 'f']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'really', 'appreciate', 'it']\n",
            "Predicted output:  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['are', 'you', 'hiding', 'something']\n",
            "Predicted output:  ['i', 'am', 'hiding', 'something']\n",
            "Word Error Rate :  0.5\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Predicted output:  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['do', 'not', 'worry']\n",
            "Predicted output:  ['do', 'not', 'worry']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'really', 'appreciate', 'it']\n",
            "Predicted output:  ['i', 'really', 'appreciate', 'it']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'really', 'appreciate', 'it']\n",
            "Predicted output:  ['i', 'really', 'appreciate', 'it']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['congratulations']\n",
            "Predicted output:  ['congratulations']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['how', 'can', 'i', 'help', 'you']\n",
            "Predicted output:  ['how', 'can', 'i', 'help', 'you']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'really', 'appreciate', 'it']\n",
            "Predicted output:  ['i', 'really', 'appreciate', 'it']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'am', 'very', 'happy']\n",
            "Predicted output:  ['i', 'am', 'very', 'happy']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['hi', 'how', 'are', 'you']\n",
            "Predicted output:  ['hi', 'how', 'are', 'you']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['are', 'you', 'hiding', 'something']\n",
            "Predicted output:  ['are', 'you', 'hiding', 'something']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['do', 'not', 'worry']\n",
            "Predicted output:  ['do', 'not', 'worry']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'am', 'very', 'happy']\n",
            "Predicted output:  ['i', 'am', 'very', 'happy']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['do', 'not', 'worry']\n",
            "Predicted output:  ['are', 'you', 'hiding', 'something']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['hi', 'how', 'are', 'you']\n",
            "Predicted output:  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'really', 'appreciate', 'it']\n",
            "Predicted output:  ['how', 'can', 'i', 'help', 'you']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['he', 'she', 'is', 'my', 'friend']\n",
            "Predicted output:  ['he', 'she', 'is', 'my', 'friend']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['congratulations']\n",
            "Predicted output:  ['congratulations']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'really', 'appreciate', 'it']\n",
            "Predicted output:  ['hi', 'how', 'are', 'you']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['do', 'not', 'worry']\n",
            "Predicted output:  ['do', 'not', 'worry']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Predicted output:  ['i', 'really', 'i', 'help', 'you']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['congratulations']\n",
            "Predicted output:  ['congratulations']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['hi', 'how', 'are', 'you']\n",
            "Predicted output:  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['hi', 'how', 'are', 'you']\n",
            "Predicted output:  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Predicted output:  ['hi', 'how', 'are', 'free', 'that']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['he', 'she', 'is', 'my', 'friend']\n",
            "Predicted output:  ['he', 'she', 'is', 'my', 'friend']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['i', 'am', 'very', 'happy']\n",
            "Predicted output:  ['are', 'you', 'hiding', 'something']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['are', 'you', 'free', 'today']\n",
            "Predicted output:  ['i', 'can', 'i', 'help', 'you']\n",
            "Word Error Rate :  1.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['congratulations']\n",
            "Predicted output:  ['congratulations']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['he', 'she', 'is', 'my', 'friend']\n",
            "Predicted output:  ['he', 'she', 'is', 'my', 'friend']\n",
            "Word Error Rate :  0.0\n",
            "-------------------------------------------------------\n",
            "Expected output :  ['can', 'you', 'repeat', 'that', 'please']\n",
            "Predicted output:  ['are', 'you', 'free', 'today', 'f']\n",
            "Word Error Rate :  0.8\n",
            "\n",
            "==========================================================\n",
            "Avarage word error rate:  0.3880952380952381\n"
          ]
        }
      ],
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=False)[0][0][\n",
        "        :, :max_length\n",
        "    ]\n",
        "    # Iterate over the results and get back the text\n",
        "    numToChar = num_to_text(results[0])\n",
        "    return numToChar\n",
        "\n",
        "def num_to_text(list_of_string):\n",
        "  numToChar=[]\n",
        "  for k in  list_of_string:\n",
        "    if str(int(k)) in num_to_text_dict :\n",
        "        numToChar.append(num_to_text_dict[str(int(k))])\n",
        "  return numToChar\n",
        "\n",
        "avg_wer=0\n",
        "\n",
        "# Check the results on test samples\n",
        "for i in range(0,len(X_test)):\n",
        "    batch_images = X_test[i]\n",
        "    batch_labels = y_test[i]\n",
        "\n",
        "    arr = np.array(batch_images)\n",
        "    arr = tf.ragged.constant([arr])\n",
        "    arr = arr.to_tensor()\n",
        "    preds = prediction_model.predict(arr)\n",
        "    pred_texts = decode_batch_predictions(preds)\n",
        "    batch_labels = num_to_text(batch_labels)\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    print(\"Expected output : \",batch_labels)\n",
        "    if len(batch_labels) > len(pred_texts):\n",
        "        while(len(batch_labels) > len(pred_texts)):\n",
        "           pred_texts.append(\"f\")\n",
        "        error = wer(batch_labels, pred_texts)\n",
        "    else:\n",
        "        while(len(batch_labels) < len(pred_texts)):\n",
        "            batch_labels.append(\"f\")\n",
        "        error = wer(batch_labels, pred_texts)\n",
        "\n",
        "    print(\"Predicted output: \",pred_texts)\n",
        "    print(\"Word Error Rate : \",error)\n",
        "    avg_wer = avg_wer + error\n",
        "\n",
        "print(\"\")\n",
        "print(\"==========================================================\")\n",
        "print(\"Avarage word error rate: \",avg_wer/(len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "gradient": {
          "id": "e1cdd2d8-6dc1-49c0-99b2-27f2bd2623a9",
          "kernelId": ""
        },
        "id": "LA2MuQhB6rk2",
        "outputId": "ea6fa77f-2e91-474d-f5d5-0992bd4b8b65"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {
          "id": "44f2dec4-9769-4fc2-8a14-a3ff12dda9f8",
          "kernelId": ""
        },
        "id": "rW7Ni7i-VLvO",
        "outputId": "0ea478fc-2d61-4c9e-ae56-d52bcacefe33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# Save trained model\n",
        "prediction_model.save('prediction_model2.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "\n",
        "# prediction_model_loaded = load_model('prediction_model.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
